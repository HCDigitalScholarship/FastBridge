{% extends "base.html" %}
{% block content %}

<body>
    <header class="masthead"
        style="background-image: url('/assets/img/stats_background_2.jpg'); background-size: cover;">
        <div class="container">
            
            <!-- Metrics -->
            <section class="mt-5">
                <div class="container p-4 mb-5" style="background-color: rgba(0, 0, 0, 0.7); border-radius: 10px; text-align: left; color: white;">

                    <style>
                        .definition-table {
                            width: 100%;
                            border-collapse: collapse;
                            margin-bottom: 3rem;
                        }
                        .definition-table td {
                            padding: 10px 15px;
                            vertical-align: top;
                        }
                        .definition-table tr:not(:last-child) {
                            border-bottom: 20px solid transparent;
                        }
                        .section-heading {
                            margin-top: 3rem;
                            margin-bottom: 1.5rem;
                            text-align: center;
                        }
                    </style>
                    <div>
                    <h1 class="section-heading">Formulas Explanation</h1>
                    <p style="text-align: center;">Here you will find the explanations for the formulas used in our metrics.</p>
                    </div>
                    <h2 class="section-heading">DEFINITIONS</h2>
                    <table class="definition-table">
                        <tbody>
                            <tr><td><strong>Word:</strong></td><td>Every different form of the word; i.e. a “token” or N. For the purposes of these analyses, proper names (e.g., Marcus, Roma) are excluded from the calculations.</td></tr>
                            <tr><td><strong>Unique Word:</strong></td><td>Total words irrespective of different forms; i.e. “types” or V; e.g. bellum and belli would be two “words” or two “tokens” but only one “unique word” or “type”.</td></tr>
                            <tr><td><strong>Word Length:</strong></td><td>The number of characters in a word; e.g. egerunt = 7. Six characters is the typical threshold in readability studies for a “long word.”</td></tr>
                            <tr><td><strong>Lexical Density:</strong></td><td>The number of lexical or content words (nouns, adjectives, verbs, and adverbs) divided by the total number of words in a text.</td></tr>
                            <tr><td><strong>Lexical Sophistication:</strong></td><td>The number of unusual or advanced words in the text, defined as those words that are not in the Diederich 1500, or around the 80% vocabulary in a typical text.</td></tr>
                            <tr><td><strong>Lexical Variation:</strong></td><td>How often individual words are repeated within a text.</td></tr>
                            <tr><td><strong>TTR:</strong></td><td>The ratio of the number of unique words (“types” or V) to the total number of words (“tokens” or N). This formula exhibits bias against shorter texts.</td></tr>
                            <tr><td><strong>CTTR or Corrected TTR:</strong></td><td>Corrected TTR divides the number of types (V) by the square-root of 2 times the number of tokens (N).</td></tr>
                            <tr><td><strong>Root TTR:</strong></td><td>Root TTR divides the number of types (V) by the square-root of the number tokens.</td></tr>
                            <tr><td><strong>LogTTR or Herdan's C:</strong></td><td>LogTTR divides the log of the number of types (V) by the log of the number of tokens (N).</td></tr>
                            <tr><td><strong>Hapax (legomenon):</strong></td><td>LogTTR divides the log of the number of types (V) by the log of the number of tokens (N).</td></tr>
                            <tr><td><strong>Bin Frequency (# on graph):</strong></td><td>The percentage of words in text selection that fall into certain frequency bins within the Diederich 1500.</td></tr>
                        </tbody>
                    </table>

                    <h2 class="section-heading">PLOTS</h2>
                    <table class="definition-table">
                        <tbody>
                            <tr><td><strong>Word Frequency:</strong></td><td>Plots a histogram of the most common words found in text selection, no exclusions.</td></tr>
                            <tr><td><strong>Cumulative Lexical Load:</strong></td><td>Assigns a lexical score to each word of the text, displays the ever-growing cumulative difficulty/load of the text.</td></tr>
                            <tr><td><strong>Linear Lexical Load:</strong></td><td>Assigns a lexical score to each word, displays the change in lexical load of the text selection.</td></tr>
                            <tr><td><strong>Bin Frequency:</strong></td><td>Displays the percentage of words in text selection that fall into certain frequency bins within the Diederich 1500.</td></tr>
                        </tbody>
                    </table>

                    <h2 class="section-heading">READABILITY FORMULAE</h2>
                    <table class="definition-table">
                        <tbody>
                            <tr><td><strong>LexR:</strong></td><td>A word that only appears once in the selection.</td></tr>
                            <tr><td><strong>PLexR:</strong></td><td>PLexR calculates a text’s readability by replacing LexR’s generic parameters for lexical high frequency and lexical sophistication with customized core and non-core vocabularies. [in development]</td></tr>
                            <tr><td><strong>ARI:</strong></td><td>= 4.71 * (Total # of Characters / Total # of Words) + (0.5 * (Total # of Words / Total # of Sentences)) - 21.43<br>&nbsp;The Automated Readability Index (ARI) scores map onto US Grade Level.</td></tr>
                            <tr><td><strong>Coleman-Liau Index:</strong></td><td>= (0.0588 * (Total # of Characters / Total # of Words) * 100) - (0.296 * ((Total # of Sentences / Total # of Words) * 100)) - 15.8<br>&nbsp;Coleman-Liau Index scores map onto US Grade Level.</td></tr>
                            <tr><td><strong>LIX:</strong></td><td>= (Total # of Words / Total # of Sentences) + (Long Words * 100 / Total # of Words)<br>&nbsp;Texts with scores greater than 40 are considered difficult.</td></tr>
                            <tr><td><strong>RIX:</strong></td><td>= (Long words / Total Sentences)<br>&nbsp;A simpler metric than LIX. Texts with scores greater than 2 are considered difficult.</td></tr>
                            <tr><td><strong>SMOG:</strong></td><td>= 1.043 * sqrt(Total # of Long Words * 30 / Total # of Sentences ) + 3.1291<br>&nbsp;SMOG scores map onto US Grade Level.</td></tr>
                            <tr><td><strong>Spache:</strong></td><td>= (0.121 * Average Sentence Length in Words) + (0.082 * Total # of Unique Unfamiliar Words [words outside the DCC Latin Core] / Total # of Unique Words) + 0.659<br>&nbsp;The Spache formula is designed for texts aimed at young readers (e.g., grades 1-3 in English). A Score of 6 would be typical of texts appropriate for US 9th Grade.</td></tr>
                        </tbody>
                    </table>
                </div>
            </section>
        </div>
    </header>
</body>

{% endblock %}
